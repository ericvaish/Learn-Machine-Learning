
LSTM Cell Block

Long Short-Term Memory (LSTM) networks are a special kind of recurrent neural network (RNN) capable of learning long-term dependencies. They were introduced to overcome the Vanishing Gradient and Vanishing Exploding problems faced by traditional RNNs.

```
LSTM Cell
|------|------|------|------|
|Forget|Input |Output|Cell  |
|Gate  |Gate  |Gate  |State |
|(0-1) |(0-1) |(0-1) |      |
|------|------|------|------|
```

Cell State : Stors the long term memory of the network.

Forget Gate : Defines the state information thats stored can be forgotten. Meaning its no longer contextually relevant.

Input Gate : Defines the new information to be added or updated to the working storage state information.

Output Gate : Out of all information stored in the state information, what information should we output.

(0-1) : Each gate is assigmed a number between 0 and 1.
0 : Gate is closed. Nothing gets through.
1 ; Gate is open. Everything gets through

eg.
1. Forget everything
2. Forget a little bit
3. Add everyhthing
4. Add nothing


Use Case:
1. Machine Translation
2. QnA Chatbots
3. Time sequence data



## How Time Series data is Trained into an LSTM Model

Time series forcasting involves training an LSTM on sequencial data. 
eg. Stock Prices, Temperature Readings, Navigational Data etc.

STEP 1 : DATA PREPROCESSING

1. Collect Sequencial Data: The dataset consists of observations recorded at regular time intervals.
2. Normalize the Data: Standardizations (zero mean, unit variance) or Min-Max scaling is applied to keep input values within a small range.
