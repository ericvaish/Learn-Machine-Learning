## Regularization

## Table of Contents
1. [Introduction](#introduction)
2. [Types of Regularization Techniques](#types-of-regularization-techniques)
   - [L1 Regularization (Lasso)](#l1-regularization-lasso)
   - [L2 Regularization (Ridge)](#l2-regularization-ridge)
   - [Dropout Regularization](#dropout-regularization)
   - [Batch Normalization](#batch-normalization)
   - [Data Augmentation](#data-augmentation)
   - [Early Stopping](#early-stopping)
4. [Comparing L1 vs L2 Regularization](#comparing-l1-vs-l2-regularization)

## Introduction
Regularization is a technique used to prevent overfitting by introducing constraints or penalties during model training. Overfitting occurs when a model learns patterns too specific to the training data, leading to poor generalization on unseen data.

## Types of Regularization Techniques

### 1. L1 Regularization (Lasso)

* L1 Regularization, a.k.a Lasso (Least Absolute Shrinkage and Selection Operator)
* It works by adding a penalty equal to the **absolute value of the weights** to the loss function. This encourages sparsity, meaning that some of the weights become exactly zero, effectively removing less important features.  

* Mathematical Formulation  
L1 regularization modifies the original loss function by adding a penalty term:

<!-- L1 Loss = (1/n) Σ | yᵢ - ŷᵢ | -->

<!-- ![L1 Loss](https://latex.codecogs.com/png.latex?L1\_Loss%20%3D%20%5Cfrac%7B1%7D%7Bn%7D%20%5Csum_%7Bi%3D1%7D%5En%20%7C%20y_i%20-%20%5Chat%7By%7D_i%20%7C) -->

        L1 = Loss + λ ∑∣w∣

![L1 Loss](/Volumes/T7 Touch (1)/MAC_MINI/Projects/Learn-Machine-Learning/Formulas/L1_loss.png)



Where:

* Loss = Original loss function (e.g., Mean Squared Error for regression, Cross-Entropy for classification)
* λ = Regularization parameter (controls the strength of L1 regularization)
* w = Model weights (coefficients)  
* ∑∣w∣ = Sum of absolute values of the weights

#### How It Works
* The larger the value of λ, the stronger the regularization effect.
* As λ increases, more weights become zero, leading to feature selection.
* When λ=0, L1 regularization has no effect, and the model behaves like a standard model without regularization.

---

### 2. L2 Regularization
### 3. L3 Regularization
### 4. L1+L2 Regularization
### 5. Dropout Regularization
### 6. Batch Regularization
### 7. Data Augmentation (Implicit Regularization)
### 5. Early Stopping
### 5. Dropout Regularization
### 5. Dropout Regularization


## Key Differences Between L1 and L2 Regularization

| Feature                 | **L1 Regularization (Lasso)**              | **L2 Regularization (Ridge)**              |
|-------------------------|--------------------------------|--------------------------------|
| **Mathematical Formula** | ( Loss + lambda sum |w| ) | \( Loss + \lambda \sum w^2 \) |
| **Penalty Type**         | Adds **absolute values** of weights as a penalty | Adds **squared values** of weights as a penalty |
| **Effect on Weights**    | Some weights become **exactly zero** (sparse solution) | Reduces all weights but does not make them zero |
| **Feature Selection**    | Yes, removes irrelevant features by setting weights to zero | No, keeps all features but reduces their impact |
| **Computational Efficiency** | Slower, since absolute value is non-differentiable | Faster, since squared terms are differentiable |
| **Use Cases**           | When **feature selection** is needed | When **all features** are useful but should be regularized |
| **Typical Applications** | Sparse models, high-dimensional data, feature selection | Preventing large weights, stable model learning |



## How to Choose the Right Regularization Technique?
| Scenario                                           | Recommended Regularization      |
|----------------------------------------------------|--------------------------------|
| Model overfits                                    | Dropout, L2 Regularization     |
| Large weights in the network                      | L2 Regularization              |
| Feature selection needed                          | L1 Regularization              |
| Model trains too slowly                           | Batch Normalization            |
| Small dataset                                     | Data Augmentation              |
| Training loss keeps decreasing but validation loss increases | Early Stopping      |



## References:

<a title="CodeCogs.com" href="https://www.codecogs.com">
<img src="https://www.codecogs.com/images/poweredbycodecogs.png" border="0"
title="CodeCogs - An Open Source Scientific Library"
alt="Powered by CodeCogs">
</a>

